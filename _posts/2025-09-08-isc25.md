---
layout: post
title: "ISC25 Student Cluster Competition"
description: My experience at ISC25 in Hamburg at the Stucent Cluster Competition.
---

In 2025 I went on [ISC25](https://isc-hpc.com/) in Hamburg, Germany. I still think this was the best event I've attended that year.

Arrival to hamburg with airport. 
(shows map.png)
(shows hamburg-to-plane.jpg)

At hamburg we stayed at a nice little hotel on this street.

Next morning I went to ISC25.

(shows me-at-isc25-front.jpg)
I was really surprised to learn just how loud supercomputers can be. 

(shows server-brr-23fps.mp4)
Mind you, in this video the supercomputer is not in-use. It is idle. That whistling noise is the idle sound. It gets a lot louder when the computer is in use.

(shows holding-the-h200-in-hands-00.jpg holding-the-h200-in-hands-01.jpg)
I had a chance to hold an actual NVIDIA H200 NVL in my hands. This thing costs $30K. It's not too heavy, almost weighs the same as a large book. 

(shows nvidia-paper-with-h200nvl.jpg)
When you buy it it comes with this cute little paper from nvidia. This'd be my most expensive unboxing experience. 

(shows complete-setup-rack-with-squid.jpg)
(shows server-rack-up-close.jpg)
There were two racks in the supercomputer. Called deuterium and tritium. Cue in our team's namesake FAUSION. I don't know who came up with the naming, but our universities FAU plus USI was mixed into name "FAUSION", and our computers were named deuterium and tritum. Very energetic naming.


(shows github-repos.png)
On SCC, there were 5 computational challenges:
1. LLM: you had to fine-tune Llama3.1 8B as fast as possible using QLora (shows llama.png).
2. HPL: In this we ran Linpack and Conjugate Gradient benchmarks. (shows hpl.png)
3. IO500: In this we tested storage speed, make files, remove files, move directories, etc (shows io500.png).
4. OpenMX: OpenMX is a Open-source package for Material eXplorer. This is simulation of molecules and atoms. (shows openmx.png)
5. SeisSol: This simulates how earthquakes work with supercomputers. (shows seissol.png)

In all tasks, we had the most pressing challenge -- never exceed 6kW power draw. If you do, you get disqualified!

(shows when-laptop-causes-noise.jpg)
We competed with 10 other teams for the tasks. We reached 3rd place and it felt just amazing. 

(shows github-repos.png)
From these tasks, I think it was OpenMX we struggled with most. The source was less-than-well documented and we had trouble getting it to run on our GPU setup. 

(shows server-rack-up-close.jpg)
It became so bad that we actually ran with fewer H200s than we had. You see each H200 has 700w power draw in total. We only had 6kW in power budget. Of that 6k we needed 2k just to run everything non-GPU, CPUs, fans, connectors. So at first we tried to use all 12 GPUS power-capped at 300W using 3.6kW. We fit 8 + 4 GPUs in two racks.

(show complete-setup-rack-with-squid.jpg)
The problem was that HPC apps kept breaking. This didn't work because we had to spread out the GPUs as 8 + 4 over two racks and GPU imbalance, 8 vs 4 somehow caused apps to crash. 

So we disassembled the rack, and reassembled it to use 4 + 4 with 400W cap getting 3.2kW power. 

We had 10 GPUs and at first we chose to use 6 + 4 setup

(shows https://youtu.be/66eYttVqrug?feature=shared&t=873)
This was our moment of victory. We were third globally and first in europe. I think we could've gotten a 2nd place too, if we were more lucky with the GPU `apptainer`. 

(shows it-can-run-doom.jpg)
We also had some fun with the racks. The supercomputer can run doom actually. 

