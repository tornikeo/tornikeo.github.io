---
layout: post
title: "Google TPUs"
description: Documentation walkthrough
categories: paper-walkthrough
---

# Introduction

1. CPUs are have one really fast processor and are bound by memory load/save speed.
2. GPUs have many tiny CPUs inside, and really large memory load/save speed.
3. TPUs don't have either. Instead, e.g. TPU v3:
    - Has two systolic ALUs of size 128 x 128. These are ALUs that are laid out in 128 x 128 grid. 

The next animation shows how the network "weights" are laid out inside a TPU. 

![](https://cloud.google.com/static/tpu/docs/images/image4_5pfb45w.gif)

And this animation shows the "systolic" movement of data inputs into TPUs:

![](https://cloud.google.com/static/tpu/docs/images/image1_2pdcvle.gif)

So, as far as I understand, these two above animations show a pariwise convolution between all weights and all inputs. The only thing that is really important here is that from the animations it seems TPUs don't re-load the used inputs into HBM. From the animations it seems each ALU passes on the inputs to the next ALU vertically after processing. HBM access is really slow. If ALUs can really pass data upstream like that, that'd make the processing much faster. Interesting indeed.

On the horizontal side, it also seems that after processing a chunk of the weights x inputs, ALU passes the accumulated results to the right-side ALU. It is interesting what exactly passes in ALU-to-ALU transfer operations.

Recommendations:
1. Reshape operations should be avoided. Shapes must stay constant. Shapes are actually "compiled", or baked into model. If you don't use that extra 8 cells, the compiled will still add those and discard them afterwards. 
1. Matrices should be as large as possible and at multiples of 8
1. Matrix multiplications are easiest to perform. Any other operation (add, sub, reshape) are slower.

## Summary

Surprise-surprise, vanilla transformers are really well-suited for google's TPUs. Especially large ones with large enough hidden dimension. I'd be fair to say that depth of the model won't matter as much as the hidden size. 

Second, it's best to use JAX for getting your model to work on a TPU. Don't use PyTorch. JAX has a better support for TPUs. Don't use TensorFlow under any circumstance. 

We know something about the heart of the machine now -- what about the rest? What about the TPU host, its disk size, case studies, etc. 

# System arch

## TPU-related Terms

1. Batch Inference: Run when available. Slow as heck.
1. TPU chip contains TensorCores (TC). Better TPUs have more/faster TCs.
    TC has many matrix-multiply units (MXUs), a vector unit and scalar unit.
    MXU is either 256x256 (v6e and later) or 128x128 multiply accumulator. MXUs are the workhorse of the TPUs. 
    TPUs do multiplication in bfloat16, and accumulation in FP32.
    Vector unit does activations and softmax. Scalar unit does control flow and mem address ops.
1. TPU cube. This is a topology of several connected TPUs. TPU is adjacent to 6 other TPUs, I guess. Starts with v4 and later TPUs.
1. Multislice is connection between several TPU slides. Of course, TPU slices communicate better than a multislice.
1. Cloud TPU ICI resiliency -- it's just a safeguard used when connecting TPU cubes with each other.
1. Queued resource -- A representation of TPU resources, used to enqueue and manage a request for a single-slice or multi-slice TPU environment. Basically, queued resource is how you stand in queue, when wanting to access a TPU. This has to be connected with "Batch Inference".
1. Single host, multi host, and sub host -- TPU "host" is a linux VM that controls a TPU. TPUs can have multiple hosts and a host can have many TPUs. A Sub-host workload would mean a process doesn't use all TPUs of the host.
1. **Slice** -- Pod slice is a collection of chips that are connected with really fast chip interconnects (ICI). Slices can have differen ICI connection shapes, this is called chip topology, and different number of chips. A slice can also be described in total number of TensorCores it has too.
1. SparseCore -- if you have a large recommendation engine that relies on embedding vectors, you likely want to use SparseCore. v5 and later chips have specialized hardware for this, called "SparseCores".
1. **TPU Pod** -- A collection of TPUs that are physically close to each other and connected with a really fast network. TPU Pods are what you will use if you want to train a large model that doesn't fit on a single TPU HBM.
1. **TPU VM or "worker"** -- A virtual machine running Linux that has access to the underlying TPUs. For practical purposes, it's an ssh terminal that can compile and run TPU based XLA code.
1. TensorCores -- these do efficient matrix multiplications. See this [ACM article](https://dl.acm.org/doi/pdf/10.1145/3360307) for details.
1. TPU versions -- TPU chip architecture changes a lot between chips. AFAIK, there's smaller incentive in keeping everything backwards-compatible, so google changes TPUs a lot between versions. 

# TPU VM images

A default vm for TPUs is `tpu-ubuntu2204-base`. There are others too. 

What's inside `tpu-ubuntu2204-base`?


# TPU versions
All TPUs have bf16 mul, fp32 accumulation.


## TPU v6e
Somewhat similar to v5e, but newer. Designed for both training and tuning.

Each v6e has 1 TC and each TC has 2 MXUs. 

1. **Single host can have 8 TPUs at most**
1. Memory/BW: **32GB**,  1640 Gbps
1. Peak perf: 918 TFLOPs
1. Max Pod size: 8960 TPUs (!!)
1. ICI BW: 3584 Gbps
1. Max pod size: 256 Chips
1. All-reduce bandwidth per Pod: 102 TB/s

**TODO how much CPU resources does host have in for v6e?**

## TPU v5p
For training. This chip is an absolute beast.

1. **Single host can have 8 TPUs at most**
1. Memory/BW: **95GB**,  2765 Gbps
1. Peak perf: 459 TFLOPs
1. Max Pod size: 8960 TPUs (!!)
1. ICI BW: 4800 Gbps

## TPU v5e

For both training and serving.

![](https://cloud.google.com/static/tpu/docs/images/TPU-host-in-v5e.png)

1. **Single host can have 8 TPUs at most**.
1. Memory/BW: **16GB**, 819 Gbps
1. All reduce BW per pod: 51.2 TB/s
1. Peak perf: 197 TFLOPs
1. ICI BW: 1600 Gbps **Interchip/intrahost is faster than CPU-to-chip**.
1. Max pod: **256 Chips**
1. Single CPU can most efficiently access TPUs 0,1,2,3.

Single-host `AcceleratorType`-s are:

1. v5litepod-1
1. v5litepod-4
1. v5litepod-8

After this, you need to use `Sax` to manage several hosts (remember, 1 host has max 8 TPUs). Then it goes to:

1. v5litepod-16, 32, etc ...

The VMs themselves:

1. `n2d-48-24-v5lite-tpu` for `v5litepod-1` has:
    - 1 v5e TPU
    - 24 CPUs
    - 48 GB CPU RAM
    - 1 NUMA
    - Disruption: High. This just means that GKE update could terminate your run possibly. You shouldn't worry about this, as long as you do periodic model checkpoints. Read up on this [here](https://cloud.google.com/kubernetes-engine/docs/concepts/handle-disruption-gpu-tpu#disruption-in-gke).
1. `n2d-192-112-v5lite-tpu` for `v5litepod-1` has:
    - 4 v5e TPUs
    - 112 CPUs
    - 192 GB CPU RAM (!)
    - 1 NUMA
    - Fewer disruptions
1.  `n2d-384-224-v5lite-tpu` for `v5litepod-1` has:
    - 8 v5e TPUs
    - 224 CPUs (!)
    - 384 GB CPU RAM (!!)
    - 2 NUMAs
    - Even fewer GKE disruptions

Larger TPUs just repeat use a number of hosts with `n2d-384-224-v5lite-tpu` VMs. **It's interesting to know** how to implement a connection between inter-host TPUs. I'm assuming 

# Regions and zones

In EU, TPUs are only in `europe-west4`. Latest v6e is only in `us-east1-d`, and `us-east5-b`.

# Supported models

There is this google [repository](https://github.com/AI-Hypercomputer/maxtext/blob/main/README.md) called `MaxText`. It's an "optimization free" TPU-based LLM training codebase that is written in python/jax and that achieves a really high TPU-usage. There are samples for training or tuning Llama2, mistral and gemma.

`MaxText` claims to be inspired by [MinGPT](https://github.com/karpathy/minGPT), which itself is a small self-contained implementation of GPT-2 model in Python. MinGPT primarily uses just two main 300 line scripts, `train.py` and `model.py`.

---

# Get started

Let's do a code walkthrough.

First download the `gcloud` shell command, and then run these commands by pasting them one-by-one in your console.

```sh
# TODO: use `gcloud projects list` and paste 'NAME' value below
export PROJECT_ID= 
# TODO: use `gcloud iam service-accounts list` and paste 'EMAIL' value below
export SERVICE_ACCOUNT=xyz-compute@developer.gserviceaccount.com 
# TODO: Come up with a simple unique name
export RESOURCE_NAME=v5litepod-1-resource

# TODO: Use browser "login with google" window to do auth
gcloud auth login # opens a browser window

# Enable TPUs
gcloud services enable tpu.googleapis.com 

# Allow TPUs to access the rest of your gcloud with this account
gcloud beta services identity create \
    --service tpu.googleapis.com \
    --project $PROJECT_ID
# This prints
# "Service identity created: `service-xyz@cloud-tpu.iam.gserviceaccount.com`"
# This email isn't needed. Ignore it.


# Stand in a queue to access a TPU node
gcloud alpha compute tpus queued-resources create $RESOURCE_NAME \
     --node-id v5litepod \
     --project $PROJECT_ID \
     --zone us-central1-a \
     --accelerator-type v5litepod-1 \
     --runtime-version v2-alpha-tpuv5-lite \
     --valid-until-duration 1d \
     --service-account $SERVICE_ACCOUNT

# check if it's your turn in the queue
gcloud alpha compute tpus queued-resources describe $RESOURCE_NAME \
     --project $PROJECT_ID \
     --zone us-central1-a

# You will see something like
# createTime: '2024-12-20T09:25:02.391983567Z'
# guaranteed: {}
# name: projects/<Project>/locations/us-central1-a/queuedResources/v5litepod-1-resource
# queueingPolicy:
#   validUntilTime: '2024-12-21T09:25:02.423775424Z'
# state:
#   state: PROVISIONING <----- REPEAT UNTIL THIS IS "ACTIVE"
# tpu:
#   nodeSpec:
#   - node:
#       acceleratorType: v5litepod-1
#       networkConfig:
#         enableExternalIps: true
#         network: default
#       queuedResource: projects/<project>/locations/us-central1-a/queuedResources/v5litepod-1-resource
#       runtimeVersion: v2-alpha-tpuv5-lite
#       schedulingConfig: {}
#       serviceAccount:
#         email: xyz-compute@developer.gserviceaccount.com
#       shieldedInstanceConfig: {}
#     nodeId: v5litepod
#     parent: projects/<project>/locations/us-central1-a

# ssh into TPU node
gcloud alpha compute tpus tpu-vm ssh v5litepod \
     --project $PROJECT_ID \
     --zone  us-central1-a
```

You will see something like:

```md
SSH key found in project metadata; not updating instance.
Using ssh batch size of 1. Attempting to SSH into 1 nodes with a total of 1 workers.
SSH: Attempting to connect to worker 0...
Warning: Permanently added 'tpu.xyz-0-voj4cz' (ED25519) to the list of known hosts.
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.5.0-1013-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

  System information as of Fri Dec 20 09:27:55 UTC 2024

  System load:  1.09521484375     Processes:                353
  Usage of /:   8.8% of 96.73GB   Users logged in:          0
  Memory usage: 2%                IPv4 address for docker0: xyz
  Swap usage:   0%                IPv4 address for ens6:    xyz

Expanded Security Maintenance for Applications is not enabled.

145 updates can be applied immediately.
88 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

7 additional security updates can be applied with ESM Apps.
Learn more about enabling ESM Apps service at https://ubuntu.com/esm


The list of available updates is more than a week old.
To check for new updates run: sudo apt update


The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

tornikeo@t1v-n-5cb9aa1d-w-0:~$
```

Yay. At this point you can access your TPU. If you are in EU like me, you will get a terminal that is really laggy, since the 
actual TPU is in US. Latency is really high. You can take a look around. 

## Look around the TPU

To see how much disk space you have, use `df -h`. I'm getting 
something like below:

```sh
tornikeo@t1v-n-5cb9aa1d-w-0:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/root        97G   14G   84G  15% /
tmpfs            24G     0   24G   0% /dev/shm
tmpfs           9.5G  1.9M  9.5G   1% /run
tmpfs           5.0M     0  5.0M   0% /run/lock
efivarfs         56K   24K   27K  48% /sys/firmware/efi/efivars
/dev/sda15      105M  6.1M   99M   6% /boot/efi
tmpfs           4.8G  4.0K  4.8G   1% /run/user/2001
```
AFAIK, this means we have around 84GB total, from which we already use 14GB. 
To list everything that's phyiscally connected to VM hardware details, use `lspci`:

```sh
tornikeo@t1v-n-5cb9aa1d-w-0:~$ lspci
00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)
00:01.0 ISA bridge: Intel Corporation 82371AB/EB/MB PIIX4 ISA (rev 03)
00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)
00:03.0 IOMMU: Advanced Micro Devices, Inc. [AMD] Milan IOMMU
00:04.0 Non-VGA unclassified device: Red Hat, Inc. Virtio SCSI
00:05.0 Unassigned class [ff00]: Google, Inc. Device 0063
00:06.0 Ethernet controller: Google, Inc. Compute Engine Virtual Ethernet [gVNIC]
00:07.0 Unclassified device [00ff]: Red Hat, Inc. Virtio RNG
```

I'm guessing the `00:05.0 Unassigned class [ff00]: Google, Inc. Device 0063` could be our TPU. Who knows?

We can learn more about CPUs, by using `hwinfo | less`:

```sh
tornikeo@t1v-n-5cb9aa1d-w-0:~$ hwinfo | less
...
----- /proc/cpuinfo -----
  processor     : 0
  vendor_id     : AuthenticAMD
  cpu family    : 25
  model         : 1
  model name    : AMD EPYC 7B13
  stepping      : 0
  microcode     : 0xffffffff
  cpu MHz               : 2450.000
  cache size    : 512 KB
  physical id   : 0
  siblings      : 24
  core id               : 0
  cpu cores     : 12
  apicid                : 0
  initial apicid        : 0
  fpu           : yes
  fpu_exception : yes
  cpuid level   : 13
  wp            : yes
  flags         : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm
  bugs          : sysret_ss_attrs null_seg spectre_v1 spectre_v2 spec_store_bypass srso
  bogomips      : 4900.00
  TLB size      : 2560 4K pages
  clflush size  : 64
  cache_alignment       : 64
  address sizes : 48 bits physical, 48 bits virtual
  power management:
```

We have 24 such AMD CPUs:

```sh
tornikeo@t1v-n-5cb9aa1d-w-0:~$ nproc
24
```

Total RAM we have is 49GB:

```sh
tornikeo@t1v-n-5cb9aa1d-w-0:~$ cat /proc/meminfo | grep MemTotal
MemTotal:       49316192 kB
```



## Install what's needed
We already have python and pip, so let's install packages inside the TPU VM. This can take 3-4 minutes.

```sh
pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html
pip install torch_xla[pallas]
pip install timm
```

At this point, you can likely run what you need. 