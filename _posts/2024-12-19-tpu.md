---
layout: post
title: "Google TPUs"
description: Documentation walkthrough
categories: paper-walkthrough
---

# Introduction

1. CPUs are have one really fast processor and are bound by memory load/save speed.
2. GPUs have many tiny CPUs inside, and really large memory load/save speed.
3. TPUs don't have either. Instead, e.g. TPU v3:
    - Has two systolic ALUs of size 128 x 128. These are ALUs that are laid out in 128 x 128 grid. 

The next animation shows how the network "weights" are laid out inside a TPU. 

![](https://cloud.google.com/static/tpu/docs/images/image4_5pfb45w.gif)

And this animation shows the "systolic" movement of data inputs into TPUs:

![](https://cloud.google.com/static/tpu/docs/images/image1_2pdcvle.gif)

So, as far as I understand, these two above animations show a pariwise convolution between all weights and all inputs. The only thing that is really important here is that from the animations it seems TPUs don't re-load the used inputs into HBM. From the animations it seems each ALU passes on the inputs to the next ALU vertically after processing. HBM access is really slow. If ALUs can really pass data upstream like that, that'd make the processing much faster. Interesting indeed.

On the horizontal side, it also seems that after processing a chunk of the weights x inputs, ALU passes the accumulated results to the right-side ALU. It is interesting what exactly passes in ALU-to-ALU transfer operations.

Recommendations:
1. Reshape operations should be avoided. Shapes must stay constant. Shapes are actually "compiled", or baked into model. If you don't use that extra 8 cells, the compiled will still add those and discard them afterwards. 
1. Matrices should be as large as possible and at multiples of 8
1. Matrix multiplications are easiest to perform. Any other operation (add, sub, reshape) are slower.

## Summary

Surprise-surprise, vanilla transformers are really well-suited for google's TPUs. Especially large ones with large enough hidden dimension. I'd be fair to say that depth of the model won't matter as much as the hidden size. 

Second, it's best to use JAX for getting your model to work on a TPU. Don't use PyTorch. JAX has a better support for TPUs. Don't use TensorFlow under any circumstance. 

We know something about the heart of the machine now -- what about the rest? What about the TPU host, its disk size, case studies, etc. 

# System arch

## TPU-related Terms

1. Batch Inference: Run when available. Slow as heck.
1. TPU chip contains TensorCores (TC). Better TPUs have more/faster TCs.
    TC has many matrix-multiply units (MXUs), a vector unit and scalar unit.
    MXU is either 256x256 (v6e and later) or 128x128 multiply accumulator. MXUs are the workhorse of the TPUs. 
    TPUs do multiplication in bfloat16, and accumulation in FP32.
    Vector unit does activations and softmax. Scalar unit does control flow and mem address ops.
1. TPU cube. This is a topology of several connected TPUs. TPU is adjacent to 6 other TPUs, I guess. Starts with v4 and later TPUs.
1. Multislice is connection between several TPU slides. Of course, TPU slices communicate better than a multislice.
1. Cloud TPU ICI resiliency -- it's just a safeguard used when connecting TPU cubes with each other.
1. Queued resource -- A representation of TPU resources, used to enqueue and manage a request for a single-slice or multi-slice TPU environment. Basically, queued resource is how you stand in queue, when wanting to access a TPU. This has to be connected with "Batch Inference".
1. Single host, multi host, and sub host -- TPU "host" is a linux VM that controls a TPU. TPUs can have multiple hosts and a host can have many TPUs. A Sub-host workload would mean a process doesn't use all TPUs of the host.
1. **Slice** -- Pod slice is a collection of chips that are connected with really fast chip interconnects (ICI). Slices can have differen ICI connection shapes, this is called chip topology, and different number of chips. A slice can also be described in total number of TensorCores it has too.
1. SparseCore -- if you have a large recommendation engine that relies on embedding vectors, you likely want to use SparseCore. v5 and later chips have specialized hardware for this, called "SparseCores".
1. **TPU Pod** -- A collection of TPUs that are physically close to each other and connected with a really fast network. TPU Pods are what you will use if you want to train a large model that doesn't fit on a single TPU HBM.
1. **TPU VM or "worker"** -- A virtual machine running Linux that has access to the underlying TPUs. For practical purposes, it's an ssh terminal that can compile and run TPU based XLA code.
1. TensorCores -- these do efficient matrix multiplications. See this [ACM article](https://dl.acm.org/doi/pdf/10.1145/3360307) for details.
1. TPU versions -- TPU chip architecture changes a lot between chips. AFAIK, there's smaller incentive in keeping everything backwards-compatible, so google changes TPUs a lot between versions. 

# TPU VM images

A default vm for TPUs is `tpu-ubuntu2204-base`. There are others too. 

What's inside `tpu-ubuntu2204-base`?


# TPU versions
All TPUs have bf16 mul, fp32 accumulation.


## TPU v6e
Somewhat similar to v5e, but newer. Designed for both training and tuning.

Each v6e has 1 TC and each TC has 2 MXUs. 

1. **Single host can have 8 TPUs at most**
1. Memory/BW: **32GB**,  1640 Gbps
1. Peak perf: 918 TFLOPs
1. Max Pod size: 8960 TPUs (!!)
1. ICI BW: 3584 Gbps
1. Max pod size: 256 Chips
1. All-reduce bandwidth per Pod: 102 TB/s

**TODO how much CPU resources does host have in for v6e?**

## TPU v5p
For training. This chip is an absolute beast.

1. **Single host can have 8 TPUs at most**
1. Memory/BW: **95GB**,  2765 Gbps
1. Peak perf: 459 TFLOPs
1. Max Pod size: 8960 TPUs (!!)
1. ICI BW: 4800 Gbps

## TPU v5e

For both training and serving.

![](https://cloud.google.com/static/tpu/docs/images/TPU-host-in-v5e.png)

1. **Single host can have 8 TPUs at most**.
1. Memory/BW: **16GB**, 819 Gbps
1. All reduce BW per pod: 51.2 TB/s
1. Peak perf: 197 TFLOPs
1. ICI BW: 1600 Gbps **Interchip/intrahost is faster than CPU-to-chip**.
1. Max pod: **256 Chips**
1. Single CPU can most efficiently access TPUs 0,1,2,3.

Single-host `AcceleratorType`-s are:

1. v5litepod-1
1. v5litepod-4
1. v5litepod-8

After this, you need to use `Sax` to manage several hosts (remember, 1 host has max 8 TPUs). Then it goes to:

1. v5litepod-16, 32, etc ...

The VMs themselves:

1. `n2d-48-24-v5lite-tpu` for `v5litepod-1` has:
    - 1 v5e TPU
    - 24 CPUs
    - 48 GB CPU RAM
    - 1 NUMA
    - Disruption: High. This just means that GKE update could terminate your run possibly. You shouldn't worry about this, as long as you do periodic model checkpoints. Read up on this [here](https://cloud.google.com/kubernetes-engine/docs/concepts/handle-disruption-gpu-tpu#disruption-in-gke).
1. `n2d-192-112-v5lite-tpu` for `v5litepod-1` has:
    - 4 v5e TPUs
    - 112 CPUs
    - 192 GB CPU RAM (!)
    - 1 NUMA
    - Fewer disruptions
1.  `n2d-384-224-v5lite-tpu` for `v5litepod-1` has:
    - 8 v5e TPUs
    - 224 CPUs (!)
    - 384 GB CPU RAM (!!)
    - 2 NUMAs
    - Even fewer GKE disruptions

Larger TPUs just repeat use a number of hosts with `n2d-384-224-v5lite-tpu` VMs. **It's interesting to know** how to implement a connection between inter-host TPUs. I'm assuming 

# Regions and zones

In EU, TPUs are only in `europe-west4`. Latest v6e is only in `us-east1-d`, and `us-east5-b`.

# Supported models

There is this google [repository](https://github.com/AI-Hypercomputer/maxtext/blob/main/README.md) called `MaxText`. It's an "optimization free" TPU-based LLM training codebase that is written in python/jax and that achieves a really high TPU-usage. There are samples for training or tuning Llama2, mistral and gemma.

`MaxText` claims to be inspired by [MinGPT](https://github.com/karpathy/minGPT), which itself is a small self-contained implementation of GPT-2 model in Python. MinGPT primarily uses just two main 300 line scripts, `train.py` and `model.py`.