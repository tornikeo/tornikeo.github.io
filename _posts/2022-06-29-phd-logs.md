---
layout: post
title:  Research logs
date: "2022-06-29 16:01:00"
description: Arguably honest logs made during my PhD struggles
tags: research logs
categories: day-to-day
---

## Jun 23, 17:40 - Tornike

### Overview
Currently, I'm actively searching for possible PhD topics - in ML. I almost feel like a [journeyman](https://en.wikipedia.org/wiki/Journeyman), having some knowledge, but not enough to prove my worth as an expert within the field. 

The one topic that got me interested today is the so called `binary neural networks` - NNs that rely on "cheaper" binary operations (XOR, OR, AND, etc.) in order to simulate the capabilities of MatMul-powered networks. There is an interesting paper on that topic, with title ["XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"](https://arxiv.org/abs/1603.05279). 
In this paper, authors *approximate* the `Conv` operation in two different ways - both relying on binary ops. 

### Brief (and probably bad) overview of XNOR-Networks paper
In the first case, which is aptly called `Binary-Weights-Network`, the inputs $$ I $$ are still usual `fp32` `[BN, C, W,  H]` matrices. However, the `Conv` operation $$ I * W $$, where $$W$$ is the kernel, is decomposed into $$ I * W \approx ( I \oplus B ) * \alpha$$, where $$B$$ is a constrained binary kernel $$B \in \{ -1, +1 \} ^ {C \times W \times H}$$ and $$\alpha$$ is a mean of $$W$$, and so $$\alpha \in R^{+}$$. 

The benefits of this approach are as follows: 
- $$B$$ can theoretically be stored in a 32-times smaller space. 
- The $$\oplus$$ can be performed using just additions and subtractions. 

The second case, the titular `XNOR-Network`, also binarizes the inputs.The approximation looks like this:

$$I * W \approx (sign(I) \otimes sign(W)) \odot K\alpha $$

Where $$\otimes$$ is the binary convolution (same as above, can be done without multiplications), the $$\odot$$ is elementwise dot product, and the $$ K\alpha $$, serves the same purpose as the $$\alpha$$ in the `Binary-Weights-Network` - to contain the scaling factors (And I arguably didn't spend enough time to understand this fully).

 The inputs are binarized just as you'd expect - just a plain $$sign(\cdot)$$ of the inputs is enough to get the job done. The authors then derive the appropriate gradients for these operations and use a `SGD` with momentum to train the network.

 Authors note that a speedup can only be gained from using this architecture when the `CPU` of the machine can't "fuse the multiplication and addition as a single cycle operation... On those CPUs, Binary-Weight-Networks does not deliver speed up".
 
 
At this point, I began searching for related code for this paper - found this [github repo](https://github.com/jiecaoyu/XNOR-Net-PyTorch), and played with it for a little bit on [kaggle](https://www.kaggle.com/code/tornikeonoprishvili/xnor-net-pytorch-mnist), and then... lost interest. Why:
- I didn't think that this CNN weights compression approach is going to be useful in the forseeable future. Yes, it does reduce size requirements, but currently, the trend is to make larger models, not smaller ones. The average consumer hardware capabilities are also increasing. If you don't believe me, take a look at the average consumer-facing software size in GB, year after year. It's increasing, getting less and less efficient (the trend even has a name - "bloatware"), while also getting *more* popular. Because hardware allows this. There is no need (yet) to make something more complex in order to save space. Which leads me to another point...
- Things are pretty damn hard already in ML :) Unless someone has a really good reason to save those extra bytes, the effort fixing the bugs that the extra complexity would bring wouldn't be worth it. Not to mention that a lot of the ideas and tools already developed for non-binary networks might not work with the binary networks without some adjustments. 

### Conclusion
Binary networks allows for up to 32x storage size reduction for Convnets and up to 64x times inference speedup for older CPU architectures. It does this by approximating Conv operation with binary operations and scaling. Both inference and training modes are available and discussed within the paper. While initially interested, I reasoned that the architecture wouldn't be popular because hardware improvements incentivise less efficient but simpler approaches in ML. 