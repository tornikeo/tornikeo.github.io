<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Tornike  Onoprishvili | Google TPUs</title>
    <meta name="author" content="Tornike  Onoprishvili" />
    <meta name="description" content="Documentation walkthrough" />
    <meta name="keywords" content="tornikeo, tornike, onoprishvili, ai, llm, portfolio, machine learning, researcher, scientist" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    <link rel="icon" type="image/x-icon" href="/assets/favicon/favicon.ico">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/assets/favicon/favicon-96x96.png">
    <link rel="apple-touch-icon" href="/assets/favicon/apple-touch-icon.png">

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tornikeo.com/tpu/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <meta name="referrer" content="no-referrer">
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://tornikeo.com/"><span class="font-weight-bold">Tornike</span>   Onoprishvili</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>

              <!-- Other pages -->
                <li class="nav-item ">
                  <a class="nav-link" href="/projects/">projects</a>
                </li>
                <li class="nav-item ">
                  <a class="nav-link" href="/publications/">publications</a>
                </li>
                
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Resume: Direct link to PDF asset -->
              <li class="nav-item">
                <a class="nav-link" href="https://raw.githubusercontent.com/tornikeo/cdn/master/assets/resume/tornikeo.pdf" target="_blank" rel="noopener noreferrer">resume
                </a>
              </li>

              <!-- Toggle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Google TPUs</h1>
    <p class="post-meta">December 19, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/category/paper-walkthrough">
          <i class="fas fa-tag fa-sm"></i> paper-walkthrough</a>  
          

    </p>
  </header>

  <article class="post-content">
    <h1 id="introduction">Introduction</h1>

<p>Google’s Tensor Processing Units (TPUs) are custom accelerators designed for large-scale machine learning workloads. Unlike CPUs and GPUs, TPUs use a unique systolic array architecture to maximize throughput for matrix operations, making them ideal for deep learning tasks. This post is a practical walkthrough of TPU architecture, setup, and usage, with tips for maximizing performance and minimizing cost.</p>

<h2 id="summary">Summary</h2>

<ul>
  <li>TPUs excel at large, static-shape matrix multiplications, making them perfect for transformer models and similar architectures.</li>
  <li>JAX is the preferred framework for TPUs; PyTorch and TensorFlow have less robust support.</li>
  <li>Efficient TPU usage requires careful attention to data layout, shape, and storage.</li>
  <li>Google Cloud provides several TPU versions, each with different capabilities and pricing.</li>
  <li>Automating setup and teardown is crucial to avoid unnecessary costs.</li>
</ul>

<h1 id="tpu-architecture-overview">TPU Architecture Overview</h1>

<ol>
  <li>CPUs have a single fast processor, limited by memory bandwidth.</li>
  <li>GPUs contain many small cores and high memory bandwidth.</li>
  <li>TPUs, e.g., v3, feature two 128x128 systolic ALUs—grids of ALUs optimized for matrix operations.</li>
</ol>

<p>The following animation shows how network weights are laid out inside a TPU:</p>

<p><img src="https://cloud.google.com/static/tpu/docs/images/image4_5pfb45w.gif" alt=""></p>

<p>And this animation shows the systolic movement of data inputs into TPUs:</p>

<p><img src="https://cloud.google.com/static/tpu/docs/images/image1_2pdcvle.gif" alt=""></p>

<p>These animations illustrate how TPUs perform pairwise convolutions between all weights and inputs. Notably, TPUs minimize slow HBM (High Bandwidth Memory) access by passing data directly between ALUs, both vertically (inputs) and horizontally (accumulated results). This design enables high throughput for large matrix multiplications.</p>

<h1 id="practical-recommendations">Practical Recommendations</h1>

<ul>
  <li>Avoid reshape operations; keep tensor shapes constant, as shapes are compiled into the model.</li>
  <li>Use large matrices with dimensions as multiples of 8 for best performance.</li>
  <li>Prefer matrix multiplications; other operations (add, sub, reshape) are slower.</li>
</ul>

<h1 id="system-architecture-and-key-terms">System Architecture and Key Terms</h1>

<h2 id="tpu-related-terms">TPU-related Terms</h2>

<ul>
  <li>
<strong>Batch Inference</strong>: On-demand, but slow.</li>
  <li>
<strong>TensorCore (TC)</strong>: Contains matrix-multiply units (MXUs), vector, and scalar units. MXUs are 128x128 or 256x256.</li>
  <li>
<strong>TPU Cube</strong>: Topology of interconnected TPUs (v4+).</li>
  <li>
<strong>Multislice</strong>: Connection between multiple TPU slices.</li>
  <li>
<strong>Queued Resource</strong>: Manages requests for TPU environments.</li>
  <li>
<strong>Host/Sub-host</strong>: Linux VM(s) controlling TPUs; a host can manage multiple TPUs.</li>
  <li>
<strong>Slice</strong>: Collection of chips connected via fast interconnects (ICI).</li>
  <li>
<strong>SparseCore</strong>: Specialized hardware for large embedding tables (v5+).</li>
  <li>
<strong>TPU Pod</strong>: Cluster of TPUs for large-scale training.</li>
  <li>
<strong>TPU VM/Worker</strong>: Linux VM with direct TPU access.</li>
  <li>
<strong>TPU Versions</strong>: Each generation differs significantly in architecture and capabilities.</li>
</ul>

<h1 id="tpu-versions-and-specs">TPU Versions and Specs</h1>

<h2 id="tpu-v6e">TPU v6e</h2>
<ul>
  <li>1 TC per chip, 2 MXUs per TC</li>
  <li>32GB memory, 1640 Gbps BW, 918 TFLOPs</li>
  <li>Max pod: 256 chips, 8960 TPUs</li>
</ul>

<h2 id="tpu-v5p">TPU v5p</h2>
<ul>
  <li>95GB memory, 2765 Gbps BW, 459 TFLOPs</li>
  <li>Max pod: 8960 TPUs</li>
</ul>

<h2 id="tpu-v5e">TPU v5e</h2>
<ul>
  <li>16GB memory, 819 Gbps BW, 197 TFLOPs</li>
  <li>Max pod: 256 chips</li>
</ul>

<h2 id="tpu-v4">TPU v4</h2>
<ul>
  <li>32GB memory, 1200 Gbps BW, 275 TFLOPs</li>
  <li>Max pod: 4096 chips</li>
</ul>

<h1 id="tpu-vm-images-and-hardware">TPU VM Images and Hardware</h1>

<p>Default VM image: <code class="language-plaintext highlighter-rouge">tpu-ubuntu2204-base</code>. Example VM specs for v5litepod-1:</p>
<ul>
  <li>1 v5e TPU, 24 CPUs, 48GB RAM</li>
  <li>Larger pods scale up CPU/RAM and NUMA nodes</li>
</ul>

<h1 id="regions-and-zones">Regions and Zones</h1>

<ul>
  <li>EU: <code class="language-plaintext highlighter-rouge">europe-west4</code>
</li>
  <li>v6e: <code class="language-plaintext highlighter-rouge">us-east1-d</code>, <code class="language-plaintext highlighter-rouge">us-east5-b</code>
</li>
</ul>

<h1 id="supported-models">Supported Models</h1>

<p>Google’s <a href="https://github.com/AI-Hypercomputer/maxtext" target="_blank" rel="noopener noreferrer">MaxText</a> repo provides optimized TPU training code for Llama2, Mistral, Gemma, etc., using JAX.</p>

<h1 id="getting-started-requesting-and-using-tpus">Getting Started: Requesting and Using TPUs</h1>

<ol>
  <li>Enable “Cloud TPU” in Google Cloud.</li>
  <li>Request quota for your desired TPU type and zone.</li>
  <li>Use <code class="language-plaintext highlighter-rouge">gcloud</code> to provision, monitor, and SSH into TPU VMs.</li>
  <li>Always delete resources when done to avoid charges.</li>
</ol>

<p>Example setup and teardown commands:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set up environment variables</span>
<span class="nb">export </span><span class="nv">PROJECT_ID</span><span class="o">=</span> <span class="c"># your project</span>
<span class="nb">export </span><span class="nv">SERVICE_ACCOUNT</span><span class="o">=</span>xyz-compute@developer.gserviceaccount.com
<span class="nb">export </span><span class="nv">RESOURCE_NAME</span><span class="o">=</span>v5litepod-1-resource

<span class="c"># Authenticate and enable TPUs</span>
gcloud auth login
gcloud services <span class="nb">enable </span>tpu.googleapis.com
gcloud beta services identity create <span class="nt">--service</span> tpu.googleapis.com <span class="nt">--project</span> <span class="nv">$PROJECT_ID</span>

<span class="c"># Request a TPU node</span>
gcloud alpha compute tpus queued-resources create <span class="nv">$RESOURCE_NAME</span> <span class="se">\</span>
     <span class="nt">--node-id</span> v5litepod <span class="se">\</span>
     <span class="nt">--project</span> <span class="nv">$PROJECT_ID</span> <span class="se">\</span>
     <span class="nt">--zone</span> us-central1-a <span class="se">\</span>
     <span class="nt">--accelerator-type</span> v5litepod-1 <span class="se">\</span>
     <span class="nt">--runtime-version</span> v2-alpha-tpuv5-lite <span class="se">\</span>
     <span class="nt">--valid-until-duration</span> 1d <span class="se">\</span>
     <span class="nt">--service-account</span> <span class="nv">$SERVICE_ACCOUNT</span>

<span class="c"># Check status</span>
gcloud alpha compute tpus queued-resources describe <span class="nv">$RESOURCE_NAME</span> <span class="se">\</span>
     <span class="nt">--project</span> <span class="nv">$PROJECT_ID</span> <span class="se">\</span>
     <span class="nt">--zone</span> us-central1-a

<span class="c"># SSH into TPU node</span>
gcloud alpha compute tpus tpu-vm ssh v5litepod <span class="se">\</span>
     <span class="nt">--project</span> <span class="nv">$PROJECT_ID</span> <span class="se">\</span>
     <span class="nt">--zone</span>  us-central1-a

<span class="c"># Delete TPU resource</span>
gcloud alpha compute tpus queued-resources delete v5litepod-1-resource <span class="se">\</span>
     <span class="nt">--project</span> <span class="nv">$PROJECT_ID</span> <span class="se">\</span>
     <span class="nt">--zone</span> us-central1-a <span class="nt">--force</span> <span class="nt">--quiet</span>
</code></pre></div></div>

<h1 id="exploring-the-tpu-vm">Exploring the TPU VM</h1>

<ul>
  <li>Check disk space: <code class="language-plaintext highlighter-rouge">df -h</code>
</li>
  <li>List hardware: <code class="language-plaintext highlighter-rouge">lspci</code>
</li>
  <li>CPU info: <code class="language-plaintext highlighter-rouge">hwinfo | less</code> or <code class="language-plaintext highlighter-rouge">nproc</code>
</li>
  <li>RAM: <code class="language-plaintext highlighter-rouge">cat /proc/meminfo | grep MemTotal</code>
</li>
</ul>

<h1 id="installing-packages">Installing Packages</h1>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>torch_xla[tpu] <span class="nt">-f</span> https://storage.googleapis.com/libtpu-releases/index.html
pip <span class="nb">install </span>torch_xla[pallas]
pip <span class="nb">install </span>timm
</code></pre></div></div>

<h1 id="automating-setup-with-startup-scripts">Automating Setup with Startup Scripts</h1>

<p>You can use a startup script to automate package installation and environment setup:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud alpha compute tpus queued-resources create <span class="nv">$RESOURCE_NAME</span> <span class="se">\</span>
     <span class="nt">--node-id</span> v5litepod <span class="se">\</span>
     <span class="nt">--project</span> <span class="nv">$PROJECT_ID</span> <span class="se">\</span>
     <span class="nt">--zone</span> us-central1-a <span class="se">\</span>
     <span class="nt">--accelerator-type</span> v5litepod-1 <span class="se">\</span>
     <span class="nt">--runtime-version</span> v2-alpha-tpuv5-lite <span class="se">\</span>
     <span class="nt">--valid-until-duration</span> 1d <span class="se">\</span>
     <span class="nt">--service-account</span> <span class="nv">$SERVICE_ACCOUNT</span> <span class="se">\</span>
     <span class="nt">--metadata</span> startup-script<span class="o">=</span><span class="s1">'#! /bin/bash
      pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html
      pip install torch_xla[pallas]
      pip install timm
      EOF'</span>
</code></pre></div></div>

<h1 id="storage-options">Storage Options</h1>

<ul>
  <li>
<strong>Boot Disk</strong>: 100GB by default; usable for small datasets.</li>
  <li>
<strong>Persistent Disk (PD)</strong>: Add for larger, persistent storage.</li>
  <li>
<strong>Cloud Storage (<code class="language-plaintext highlighter-rouge">gs://</code>)</strong>: Unlimited size, but slower than PD.</li>
  <li>
<strong>GCSFUSE</strong>: Mount a bucket as a local directory for easy access.</li>
  <li>
<strong>Filestore</strong>: High-performance, but minimum 1TB.</li>
</ul>

<h1 id="training-and-inference">Training and Inference</h1>

<ul>
  <li>Use JAX for best TPU support.</li>
  <li>For multi-host pods, use <code class="language-plaintext highlighter-rouge">--worker=all</code> to run commands on all VMs.</li>
  <li>Always match JAX/Flax versions to avoid compatibility issues.</li>
</ul>

<h1 id="cost-and-quota-considerations">Cost and Quota Considerations</h1>

<ul>
  <li>v5p and v6e chips are expensive and may require quota increases.</li>
  <li>Always automate resource deletion to avoid unexpected charges.</li>
</ul>

<h1 id="final-thoughts">Final Thoughts</h1>

<p>TPUs offer unmatched performance for large-scale deep learning, but require careful setup and management. By understanding the architecture, using the right frameworks, and automating your workflow, you can maximize both performance and cost-efficiency.</p>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        <!-- &copy; Copyright 2025 Tornike  Onoprishvili.  -->Last updated: October 13, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootstrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>
  <script src="/assets/js/spinner.js"></script>
  <script src="/assets/js/image_comparison.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <script>
    window.va = window.va || function () { (window.vaq = window.vaq || []).push(arguments); };
  </script>
  <script defer src="/_vercel/insights/script.js"></script>
  </body>
</html>

