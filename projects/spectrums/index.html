<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Tornike  Onoprishvili | SpectruMS: Foundation Model for Mass Spectrometry</title>
    <meta name="author" content="Tornike  Onoprishvili" />
    <meta name="description" content="AI model for mass spectrometry data." />
    <meta name="keywords" content="tornikeo, tornike, onoprishvili, ai, llm, portfolio, machine learning, researcher, scientist" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22https://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tornikeo.com/projects/spectrums/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <meta name="referrer" content="no-referrer">
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://tornikeo.com/"><span class="font-weight-bold">Tornike</span>   Onoprishvili</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>

              <!-- Other pages -->
                <li class="nav-item ">
                  <a class="nav-link" href="/projects/">projects</a>
                </li>
                <li class="nav-item ">
                  <a class="nav-link" href="/publications/">publications</a>
                </li>
                
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Resume: Direct link to PDF asset -->
              <li class="nav-item">
                <a class="nav-link" href="https://raw.githubusercontent.com/tornikeo/cdn/master/assets/resume/tornikeo.pdf" target="_blank" rel="noopener noreferrer">resume
                </a>
              </li>

              <!-- Toggle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
<div class="post">

  <header class="post-header">
    <h1 class="post-title">SpectruMS: Foundation Model for Mass Spectrometry</h1>
    <p class="post-description">AI model for mass spectrometry data.</p>
  </header>

  <article>
    <p>AI training is similar to file compression (think <code class="language-plaintext highlighter-rouge">.zip</code>). Both tools make large files smaller. The added benefit of AI is that it makes the compressed information more easily searchable. The downside is that training AI is nothing like right clicking and <code class="language-plaintext highlighter-rouge">archive</code>-ing a folder.</p>

<p><a href="https://github.com/tornikeo/cdn/raw/master/assets/spectrums/iccs_presentation.pdf" target="_blank" rel="noopener noreferrer">SpectruMS</a> was a large language model at Pangea Bio that <strong>compressed</strong> <a href="https://gnps.ucsd.edu/" target="_blank" rel="noopener noreferrer">GNPS</a>, <a href="https://massbank.eu/" target="_blank" rel="noopener noreferrer">Massbank</a>, <a href="https://pubchem.ncbi.nlm.nih.gov/" target="_blank" rel="noopener noreferrer">PubChem</a> into a single AI model. SpectruMS compressed petabytes of chemical data into a single 5GB <a href="https://huggingface.co/docs/transformers/en/model_doc/bart" target="_blank" rel="noopener noreferrer">BART</a> model:</p>

<p><img src="https://raw.githubusercontent.com/tornikeo/cdn/master/assets/spectrums/spectrums-squish.svg" alt=""></p>

<p>At <a href="https://www.pangeabio.com/" target="_blank" rel="noopener noreferrer">Pangea Bio</a>, SpectruMS was developed essentially from scratch. Every step, data curation, AI training, babysitting the <a href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit" target="_blank" rel="noopener noreferrer">TPU</a>, deployment and serving on AWS was done in-house. What follows is my day-to-day experience of dealing with this project and my recommendations on approaching this.</p>

<h2 id="define-metrics-early">Define metrics early</h2>

<p>Really. Metrics should be defined on day one, and implemented on day two. For instance, the metric was <code class="language-plaintext highlighter-rouge">Top-5 accuracy</code>. An entire week was wasted before this was implemented.</p>

<p>When a metric appears, it should come with two things:</p>

<ul>
  <li>Evaluation dataset</li>
  <li>Baseline model(s)</li>
</ul>

<p>Baseline models give a sense of what the laziest solution could look like. If a random model can be made with a <code class="language-plaintext highlighter-rouge">Top-5 accuracy</code> of 5%, then actually training a model with 6% <code class="language-plaintext highlighter-rouge">Top-5 accuracy</code> is not that impressive. There’s a bug somewhere in training.</p>

<p>Second is the evaluation dataset. Only two things matter for evaluation dataset:</p>
<ul>
  <li>AI in training never sees the evaluation dataset.</li>
  <li>Evaluation dataset is <em>really</em> different from training dataset.</li>
</ul>

<h2 id="settle-for-the-laziest-training-approach">Settle for the laziest training approach</h2>

<p>What is the laziest training approach? It’s the one that trains an LLM over natural language. This was what worked <em>surprisingly</em> well in practice, and it was utterly lazy because there’s a <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py" target="_blank" rel="noopener noreferrer">ton of</a> <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py" target="_blank" rel="noopener noreferrer">ready-to-use</a> <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_fim.py" target="_blank" rel="noopener noreferrer">scripts</a> that train an LLM on every conceivable hardware - GPUs, TPUs, XPUs, you name it.</p>

<p>But wasn’t this about training a foundation model for mass spectrometry? Indeed! But this mass spectrometry training goal was reframed into a language modelling problem. And it worked surprisingly well. Let me explain:</p>

<p><img src="https://raw.githubusercontent.com/tornikeo/cdn/master/assets/cosine_greedy/spectrums-lazy.svg" alt=""></p>

<p>The problem of predicting <code class="language-plaintext highlighter-rouge">mass spectra -&gt; chemical</code> was reframed into a <a href="https://huggingface.co/docs/transformers/en/tasks/question_answering" target="_blank" rel="noopener noreferrer">question-answering problem</a>. Basically, both the <code class="language-plaintext highlighter-rouge">mass spectra</code> and the <code class="language-plaintext highlighter-rouge">chemical</code> were turned into long texts, and for each <code class="language-plaintext highlighter-rouge">mass spectra</code> as input, the model was trained to output the <code class="language-plaintext highlighter-rouge">chemical</code> as deepSMILES as text. A simple text-to-text model.</p>

<p>Why did this make sense? Because writing your own training pipeline and making it efficient is a lot of effort (weeks, months even) and a lot of wasted cash. And it makes a lot of sense to lean on the existing open-source tools. If the AI problem can be reframed as a language modelling problem, weeks of development time can be saved. And this was exactly what was done at Pangea.</p>

<p>The training was reframed into basically the <a href="https://huggingface.co/learn/llm-course/en/chapter7/7" target="_blank" rel="noopener noreferrer">fine-tuning for question-answering</a> tutorial. And the <a href="https://huggingface.co/docs/transformers/en/model_doc/bart" target="_blank" rel="noopener noreferrer">BART</a> <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py" target="_blank" rel="noopener noreferrer">masked language modelling</a> pretraining over a corpus of stringified MS/MS data and chemical deepSMILES was done.</p>

<p>A BART model architecture was chosen for this task instead of a <a href="https://huggingface.co/docs/transformers/en/model_doc/openai-gpt" target="_blank" rel="noopener noreferrer">GPT</a>, even though GPT would’ve been (in my opinion) more efficient and easier. In the end, what mattered most was the quality and amount of data and not the model architecture. More on that later.</p>

<p>In essence, the inference looked like this function:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">msms_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
  <span class="n">msms_tokens</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">msms_string</span><span class="p">)</span>
  <span class="n">prediction</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">'</span><span class="s">&lt;|begin_chemical|&gt;</span><span class="sh">'</span><span class="p">)</span>
  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">next_token</span> <span class="o">+=</span> <span class="nf">model</span><span class="p">(</span><span class="n">msms_tokens</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">next_token</span> <span class="o">==</span> <span class="sh">'</span><span class="s">&lt;|end_chemical|&gt;</span><span class="sh">'</span><span class="p">:</span>
      <span class="k">break</span>
    <span class="n">prediction</span> <span class="o">+=</span> <span class="n">next_token</span>
  <span class="k">return</span> <span class="n">prediction</span>
</code></pre></div></div>

<p>To get the <code class="language-plaintext highlighter-rouge">Top-5 accuracy</code>, the model was simply sampled with nonzero temperature to create random outputs. If the exact correct string was within the first 5 tries, that was a +1 score.</p>

<h2 id="tpu-training-sucks">TPU training sucks</h2>

<p>Google is an amazing company that specializes in creating amazingly convoluted tools. TPU is Google’s chip for training AI, but using a TPU had to be one of the most painful experiences an AI engineer could be subjected to. The second biggest problem with TPUs was just how expensive they were to use.</p>

<p>TPUs were slow to start, the errors were (if logs could even be accessed) cryptic, and software written for TPU training was useless for training on anything <em>but</em> Google’s TPUs.</p>

<p><img src="https://raw.githubusercontent.com/tornikeo/cdn/master/assets/cosine_greedy/spectrums-tpus.svg" alt=""></p>

<p>The TPU situation was so bad that even though the team was offered some $100k in Google cloud credits, the team lead still decided to move away from using TPUs. Even the fine-tuning part for question-answering (read: question-in-ms/ms answer in deepSMILES) was done away from TPUs and on an A100 instance on Google Cloud.</p>

<h2 id="it-is-easy-to-burn-a-lot-of-cash">It is easy to burn a lot of cash</h2>

<p>And not only on GPUs or TPUs, mind you. When working with a lot of data (there was a petabyte or so of it), it was easy to accidentally burn a lot of cash with a single press of a button. Taking your money out of the bank, covering it with gasoline and setting it on fire would be slower. Like, it would take more effort per dollar to destroy your money than that.</p>

<p><img src="https://raw.githubusercontent.com/tornikeo/cdn/master/assets/cosine_greedy/aws-burning.svg" alt=""></p>

<p>In one accident, $2.5k went aflame on AWS, because of S3. Essentially, a lot of data was written into an S3 Glacier Deep Archive storage. During data processing, one of the workflows accidentally read a large portion of that data. On S3 Glacier, the pricing was not for storage but per GB read. The workflow read $2.5k worth of data. The billing for that day looked like a Dirac function:</p>

<p><img src="https://raw.githubusercontent.com/tornikeo/cdn/master/assets/cosine_greedy/aws-burning(1).svg" alt=""></p>

<p>Ouch.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Training the foundation model for Pangea Bio was one of my best achievements in 2025. I trained a large language model for Pangea Bio from scratch. The model is trained to predict the identity of mass spectra (answering the question “what chemical made this spectrum?”). This <code class="language-plaintext highlighter-rouge">top-n</code> classification problem was reformulated into language modelling and question answering tasks and training logic was built around this. This model easily smashed the previous internal metric for model performance and in this process I got so much practical experience. That kind of experience is going to stay very valuable precisely because of how much resource is demands. If you are considering working at Pangea Bio, I can only fully recommend that – <a href="https://www.linkedin.com/in/tornikeo/" target="_blank" rel="noopener noreferrer">please reach out on LinkedIn</a> and I can tell you more about my <em>very good</em> experience working there.</p>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        <!-- &copy; Copyright 2025 Tornike  Onoprishvili.  -->Last updated: October 02, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootstrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>
  <script src="/assets/js/spinner.js"></script>
  <script src="/assets/js/image_comparison.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

