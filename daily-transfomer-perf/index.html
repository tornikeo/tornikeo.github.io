<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Tornike  Onoprishvili | Daily scribbles - on choosing right LLM shapes for GPUs</title>
    <meta name="author" content="Tornike  Onoprishvili" />
    <meta name="description" content="tornikeo is a machine learning researcher, this is his website
" />
    <meta name="keywords" content="tornikeo, tornike, onoprishvili, ai, llm, portfolio, machine learning, researcher, scientist" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    <link rel="icon" type="image/x-icon" href="/assets/favicon/favicon.ico">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/assets/favicon/favicon-96x96.png">
    <link rel="apple-touch-icon" href="/assets/favicon/apple-touch-icon.png">

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tornikeo.com/daily-transfomer-perf/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <meta name="referrer" content="no-referrer">
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://tornikeo.com/"><span class="font-weight-bold">Tornike</span>   Onoprishvili</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>

              <!-- Other pages -->
                <li class="nav-item ">
                  <a class="nav-link" href="/projects/">projects</a>
                </li>
                <li class="nav-item ">
                  <a class="nav-link" href="/publications/">publications</a>
                </li>
                
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Resume: Direct link to PDF asset -->
              <li class="nav-item">
                <a class="nav-link" href="https://raw.githubusercontent.com/tornikeo/cdn/master/assets/resume/tornikeo.pdf" target="_blank" rel="noopener noreferrer">resume
                </a>
              </li>

              <!-- Toggle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Daily scribbles - on choosing right LLM shapes for GPUs</h1>
    <p class="post-meta">January 22, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/category/daily-scribbles">
          <i class="fas fa-tag fa-sm"></i> daily-scribbles</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>A100 memory hierarchy - what’s are “perfect” model shape for A100?
A100 memory hierarchy - what’s are optimal transformer shapes for NVIDIA GPUs?</p>

<p>There’s a thing called <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a>. Inference and serving engine for LLMs.</p>

<p>vLLM was built around <strong>PagedAttention</strong> algorithm. Introduced in <a href="https://arxiv.org/abs/2309.06180" target="_blank" rel="noopener noreferrer">paper</a>. What’s a <strong>KV cache</strong>?</p>

<blockquote>
  <p>KV cache. This is a method for better inference performance. From <a href="https://huggingface.co/blog/kv-cache-quantization" target="_blank" rel="noopener noreferrer">HF</a>. When you are generating long text, a typical autoreg transformer will predict a token by looking at all previous 999 tokens. Then it will predict next token, by looking at previous token and previous 999 tokens. Maybe older tokens could be reused somehow?</p>
</blockquote>

<p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv_cache_quantization/kv-cache-optimization.png" alt=""></p>

<p>Basically, queries grow downwards (new token = new row), and keys grow leftwards (new token = new column), and values also grow downwards (new token = new row). For processing new token, we only need last row of Q, but full K, V, due to how matrix multiplication works. <em>But</em>, we can simply restore previous K, V, instead of re-computing them. Extra memory, but less compute.</p>

<blockquote>
  <p>PagedAttention explained on <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention" target="_blank" rel="noopener noreferrer">HF</a>, is a method for inference performance. It optimizes the KV cache, by using lessons from how OS hanldes memory Paging.</p>
</blockquote>

<p>Amazon’s LLM, rufus was cooked up on top of vLLM.</p>

<p>vLLM supports models up to scale of 405B params, LLAMA 3.1.</p>

<p>Serving is simple <code class="language-plaintext highlighter-rouge">pip install vllm</code> and <code class="language-plaintext highlighter-rouge">vllm serve meta-llama/Llama-3.1-8B</code>. 
As a python package <code class="language-plaintext highlighter-rouge">vllm</code> has a simple-ish programming API for querying models.</p>

<p>Back to original article.</p>

<p><strong>Mixture of Experts</strong>? <strong>Speculative Decoding</strong>?</p>

<p>There’s the claim: “the most foundamental fact that transformer inference is memory bound”. Let’s see.</p>

<p>===</p>

<p>Preliminaries. GPU memory architecture. A100 80GB SXM has 108 Streaming multiprocessors (SMs), 40MB L2 cache.</p>

<p>When doing inference i.e. <code class="language-plaintext highlighter-rouge">model.generate(prompt)</code> these things happen:</p>
<ol>
  <li>Load layer matrices from HBM to L2 to SM.</li>
  <li>Do matmul, and use tensor cores</li>
</ol>

<p>Loading part takes much longer than tensor core part.</p>

<p>Suppose we have A100:</p>
<ul>
  <li>108 SM, DRAM 80 G, 40M L2 cache</li>
  <li>bf16 tensor core: 312 tflops</li>
  <li>DRAM mem bw, 2.039 TB/s</li>
</ul>

<p>If model is larger than 80GB, it’s split up:</p>
<ul>
  <li>Connection by NVLink 300GB/s = 0.3 T/sec</li>
</ul>

<p>See the problem? 312 TFLOPS » 2.03 TB/s » 0.3 TB/s  » 0.006 TB/s</p>

<p>This seems to show that memory is the main bottleneck. Or is it?</p>

<p>A100 prefers doing 312 operations per each 2.039 loaded bytes. How on <em>Earth</em> can you do 312 operations for approximately 2 bytes?</p>

<p>The matrix multiplication with 2 N-square matrices, has arithmetic intensity proportional to N. Therefore, larger matrix sizes might actually be compute bound. This also means that there’s some sweet-spot matmul size that the GPU likes most.</p>

<p>This also means that any op that is elementwise (e.g. activaions) will always be memory bound. For example:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">nn.Linear(1024, 4096)</code>, batch size 512, is compute bound</li>
  <li>
<code class="language-plaintext highlighter-rouge">nn.Linear(1024, 4096)</code>, batch size 1, is memory bound</li>
</ul>

<p><strong>Kernel fusion</strong> works between an <code class="language-plaintext highlighter-rouge">nn.Linear</code> and a <code class="language-plaintext highlighter-rouge">nn.Relu</code>, or any other activation. Since activation is mem bound, and elementwise, we can immediately apply relu inside CUDA kernel, before returning output to HBM.</p>

<p>Aha. Interesting bit. About online inference. Throughput vs latency:</p>
<ul>
  <li>Offline: if we are evaluating the model (offline), and no user waits at the end of the screen, we should increase batch size.</li>
  <li>Online: if user is waiting, the optimal speed to generate next token is the average human read speed, in tokens. Otherwise human might complain.</li>
</ul>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        <!-- &copy; Copyright 2025 Tornike  Onoprishvili.  -->Last updated: October 13, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootstrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>
  <script src="/assets/js/spinner.js"></script>
  <script src="/assets/js/image_comparison.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <script>
    window.va = window.va || function () { (window.vaq = window.vaq || []).push(arguments); };
  </script>
  <script defer src="/_vercel/insights/script.js"></script>
  </body>
</html>

