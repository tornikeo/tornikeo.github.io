{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../_posts/2022-07-07-ml-dev-logs.md'),\n",
       " PosixPath('../_posts/2022-06-29-phd-logs.md'),\n",
       " PosixPath('../_posts/2022-03-02-correct-mindset.md'),\n",
       " PosixPath('../_site/README.md'),\n",
       " PosixPath('../_site/CONTRIBUTING.md'),\n",
       " PosixPath('../_projects/shapy.md'),\n",
       " PosixPath('../_projects/serverless_diffusion.md'),\n",
       " PosixPath('../_projects/cloudseg.md'),\n",
       " PosixPath('../_projects/atomic_arch.md'),\n",
       " PosixPath('../_projects/pose_estim.md'),\n",
       " PosixPath('../_projects/tox21.md'),\n",
       " PosixPath('../_projects/repnet.md'),\n",
       " PosixPath('../_projects/swinir_superres.md'),\n",
       " PosixPath('../_projects/vimage.md'),\n",
       " PosixPath('../_projects/sanet.md'),\n",
       " PosixPath('../_projects/arxiv_gpt.md'),\n",
       " PosixPath('../_projects/deepdream.md'),\n",
       " PosixPath('../_news/announcement_6.md'),\n",
       " PosixPath('../_news/announcement_7.md'),\n",
       " PosixPath('../_news/announcement_12.md'),\n",
       " PosixPath('../_news/announcement_4.md'),\n",
       " PosixPath('../_news/announcement_10.md'),\n",
       " PosixPath('../_news/announcement_11.md'),\n",
       " PosixPath('../_news/announcement_5.md'),\n",
       " PosixPath('../_news/announcement_9.md'),\n",
       " PosixPath('../_news/announcement_2.md'),\n",
       " PosixPath('../_news/announcement_8.md'),\n",
       " PosixPath('../_news/announcement_3.md'),\n",
       " PosixPath('../_pages/about.md'),\n",
       " PosixPath('../_pages/resume.md'),\n",
       " PosixPath('../_pages/publications.md'),\n",
       " PosixPath('../_pages/projects.md'),\n",
       " PosixPath('../_pages/dropdown.md')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "\n",
    "target = r'https://i.imgur.com/\\w+.\\w+'\n",
    "all_mkd_files = list(Path('..').glob('_*/*.md'))\n",
    "all_mkd_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project tornikeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blob = bucket.blob('testing.txt')\n",
    "blob.upload_from_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:15<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests, urllib\n",
    "from urllib import parse\n",
    "from tqdm import tqdm\n",
    "client = storage.Client()\n",
    "url_map = {}\n",
    "for file in tqdm(all_mkd_files):\n",
    "    text = file.open('r').read()\n",
    "    found_urls = re.findall(target, text)\n",
    "    for url in found_urls:\n",
    "        bucket = storage.Bucket(client=client, name='tornikeo-portfolio-cdn')\n",
    "        p = parse.urlparse(url)\n",
    "        blob = bucket.blob(f\"imgur/{Path(p.path).name}\")\n",
    "        url_map[url] = blob.public_url\n",
    "        if not blob.exists():\n",
    "            blob.upload_from_string(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://i.imgur.com/ld3F1oB.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/ld3F1oB.png',\n",
       " 'https://i.imgur.com/YamQGoe.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/YamQGoe.png',\n",
       " 'https://i.imgur.com/b9fn695.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/b9fn695.png',\n",
       " 'https://i.imgur.com/37z17Eu.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/37z17Eu.png',\n",
       " 'https://i.imgur.com/kSGazUz.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/kSGazUz.png',\n",
       " 'https://i.imgur.com/PaEd4hK.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/PaEd4hK.png',\n",
       " 'https://i.imgur.com/eLcAtDA.mp4': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/eLcAtDA.mp4',\n",
       " 'https://i.imgur.com/IirfLVw.gif': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/IirfLVw.gif',\n",
       " 'https://i.imgur.com/DsfbqwK.gif': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/DsfbqwK.gif',\n",
       " 'https://i.imgur.com/8NWyAwQ.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/8NWyAwQ.png',\n",
       " 'https://i.imgur.com/07ylZV6.mp4': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/07ylZV6.mp4',\n",
       " 'https://i.imgur.com/TV10o0g.mp4': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/TV10o0g.mp4',\n",
       " 'https://i.imgur.com/5ArYydI.jpg': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/5ArYydI.jpg',\n",
       " 'https://i.imgur.com/BlWeOP8.jpg': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/BlWeOP8.jpg',\n",
       " 'https://i.imgur.com/vdXtHTB.jpg': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/vdXtHTB.jpg',\n",
       " 'https://i.imgur.com/dGiiSdo.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/dGiiSdo.png',\n",
       " 'https://i.imgur.com/gvVCAmX.mp4': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/gvVCAmX.mp4',\n",
       " 'https://i.imgur.com/OeCjQCn.jpeg': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/OeCjQCn.jpeg',\n",
       " 'https://i.imgur.com/AluU5Um.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/AluU5Um.png',\n",
       " 'https://i.imgur.com/N9opu9d.gif': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/N9opu9d.gif',\n",
       " 'https://i.imgur.com/9pZ9XHs.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/9pZ9XHs.png',\n",
       " 'https://i.imgur.com/ahwKchR.mp4': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/ahwKchR.mp4',\n",
       " 'https://i.imgur.com/MzUbXIp.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/MzUbXIp.png',\n",
       " 'https://i.imgur.com/4r6AxEC.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/4r6AxEC.png',\n",
       " 'https://i.imgur.com/Z78wJUV.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/Z78wJUV.png',\n",
       " 'https://i.imgur.com/c4xxMg4.gif': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/c4xxMg4.gif',\n",
       " 'https://i.imgur.com/Vv9MkE6.png': 'https://storage.googleapis.com/tornikeo-portfolio-cdn/imgur/Vv9MkE6.png'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 4053.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in tqdm(all_mkd_files):\n",
    "    text = file.open('r').read()\n",
    "    found_urls = re.findall(target, text)\n",
    "    replaced = text\n",
    "    for url in found_urls:\n",
    "        if url in url_map.keys():\n",
    "            replaced = re.sub(url, url_map[url], replaced)\n",
    "    # print(replaced)\n",
    "    file.open('w').write(replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24004"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('../_posts/something.md').write_text(replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../_posts/2022-07-07-ml-dev-logs.md'),\n",
       " PosixPath('../_posts/2022-06-29-phd-logs.md'),\n",
       " PosixPath('../_posts/2022-03-02-correct-mindset.md'),\n",
       " PosixPath('../_site/README.md'),\n",
       " PosixPath('../_site/CONTRIBUTING.md'),\n",
       " PosixPath('../_projects/shapy.md'),\n",
       " PosixPath('../_projects/serverless_diffusion.md'),\n",
       " PosixPath('../_projects/cloudseg.md'),\n",
       " PosixPath('../_projects/atomic_arch.md'),\n",
       " PosixPath('../_projects/pose_estim.md'),\n",
       " PosixPath('../_projects/tox21.md'),\n",
       " PosixPath('../_projects/repnet.md'),\n",
       " PosixPath('../_projects/swinir_superres.md'),\n",
       " PosixPath('../_projects/vimage.md'),\n",
       " PosixPath('../_projects/sanet.md'),\n",
       " PosixPath('../_projects/arxiv_gpt.md'),\n",
       " PosixPath('../_projects/deepdream.md'),\n",
       " PosixPath('../_news/announcement_6.md'),\n",
       " PosixPath('../_news/announcement_7.md'),\n",
       " PosixPath('../_news/announcement_12.md'),\n",
       " PosixPath('../_news/announcement_4.md'),\n",
       " PosixPath('../_news/announcement_10.md'),\n",
       " PosixPath('../_news/announcement_11.md'),\n",
       " PosixPath('../_news/announcement_5.md'),\n",
       " PosixPath('../_news/announcement_9.md'),\n",
       " PosixPath('../_news/announcement_2.md'),\n",
       " PosixPath('../_news/announcement_8.md'),\n",
       " PosixPath('../_news/announcement_3.md'),\n",
       " PosixPath('../_pages/about.md'),\n",
       " PosixPath('../_pages/resume.md'),\n",
       " PosixPath('../_pages/publications.md'),\n",
       " PosixPath('../_pages/projects.md'),\n",
       " PosixPath('../_pages/dropdown.md')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mkd_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\nlayout: post\\ntitle:  MLDev logs - day-to-day log\\ndate: \"2022-07-07 11:01:00\"\\ndescription: Arguably honest logs while learning ML development\\ntags: Machine Learning, Development\\ncategories: day-to-day\\n---\\n\\n\\n## 2022, Jul 7th, 12:10 - TornikeO\\n\\n### The issue of creating samples\\n\\nHere\\'s a quick tip: Sharing a simple link to an impressive ML [showcase website]({{site.baseurl}}/projects/5_project/) you built from scratch, will increase the chances of you getting hired. Great. So, you want to get a website up and running right away. That\\'s also great, however, unless you have a steady stream of disposable income, you wouldn\\'t want to have a **permanent** **dedicated** **highcpu** **VM** running all the time in the cloud, when typically, you only need to run the sample *once* or *twice* in a week!\\n\\n<div class=\"row mt-3\" style=\"justify-content:center;\">\\n    <div class=\"col-sm-8 mt-3 mt-md-0\" >\\n        {% include figure.html path=\"https://i.imgur.com/ld3F1oB.png\" class=\"img-fluid rounded z-depth-1\" zoomable=false %}\\n    </div>\\n</div>\\n<div class=\"caption\" >\\n    IT. IS. EXPENSIVE.\\n</div>\\n\\nWe are talking about ML samples, mind you. Even the moderately impressive ones require hardware acceleration - GPUs running 24/7. Yeah. It\\'s going to get expensive. \\n\\n### User\\'s device as a service (UDaaS)\\nJust run it on the user\\'s device. \\n\\nPros:\\n- No need to pay for anything. \\n- Godlike scaling. \\n- No data shared, no privacy issues.\\n\\nCons:\\n- Increased dev time - you need to rewrite model into a Javascript-compatible format (and, no, the TFJS converter doesn\\'t work 99% of the time). \\n- Slow prediction times for users without expensive hardware. \\n- It becomes easy to steal your intellectual property (assuming you owned it in the first place)\\n\\nWe are talking about web apps, by the way. Android apps could easily circumvent the `Slow prediction` and `Stealing IP` parts. I\\'ve yet to write an ML android app, so this is unreliable.\\n\\n### Cloud \\nRun it in the cloud!\\n\\nPros:\\n- Easy development with containers.\\n- Handle any complex model quickly.\\n\\nCons:\\n- Cloud is expensive. Especially with accelerators.\\n- Data privacy issues.\\n- Scalability (shouldn\\'t be an issue once enough users join)\\n- Cloud providers try to lock you in their infrastructure.\\n\\nHowever, the price and scalability issues can be fixed with correct tools! \\n\\n\\n### Enter Kubeflow + Vertex AI pipelines\\n\\n<div class=\"row mt-3\" style=\"justify-content:center;\">\\n    <div class=\"col-sm-8 mt-3 mt-md-0\" >\\n        {% include figure.html path=\"https://www.stackhpc.com/images/kubeflow.png\" class=\"img-fluid rounded z-depth-1\" zoomable=false %}\\n    </div>\\n</div>\\n<div class=\"caption\" >\\n    Give me my scalable-to-zero VM!\\n</div>\\n\\nSay it with me:\\n\\n**SERVERLESS SCALABLE-TO-ZERO GPU-ACCELERATED SERVICE!**  \\n**SERVERLESS SCALABLE-TO-ZERO GPU-ACCELERATED SERVICE!**  \\n\\nBecause that\\'s what we need. We don\\'t need to keep any state in the model\\'s memory - \"serverless\". We need the instance of the model to stop running when there are no requests incoming - \"scalable-to-zero\". We need quick access to GPU when some input *does* come through - \"gpu-accelerated\". Finally, we need all of this in an accessible service-like package - imagine how easy it is to write a GCP cloud run function in python and flask - \"service\". That\\'s it. This solves *checks the notes*, all the problems with using cloud as listed above. You just pay for what you use, and unless you manually cache incoming data somewhere, isn\\'t going to cause privacy issues.\\n\\nA few years back, especially before the wide adoption of the NVIDIA Ampere GPUs, this was a hard task. Look at these [poor](https://towardsdatascience.com/searching-the-clouds-for-serverless-gpu-597b34c59d55) [souls](https://www.reddit.com/r/MachineLearning/comments/lpld92/d_serverless_solutions_for_gpu_inference_if/). \\n\\nBut now, things have changed. GCP rolled out [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction). \\n\\n\\n<div class=\"row mt-3\" style=\"justify-content:center;\">\\n    <div class=\"col-sm-8 mt-3 mt-md-0\" >\\n        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/Jrh-QLrVCvM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n    </div>\\n</div>\\n<div class=\"caption\" >\\n    Give me my scalable-to-zero VM!\\n</div>\\n\\n### Current progress\\n\\nCurrently, I managed to get a hello-world sample KFP (kubeflow pipeline) working, on google\\'s Vertex AI pipelines, and the dashboard suggests that the instance is only running when someone queries it. See:\\n\\n<div class=\"row mt-3\" style=\"justify-content:center;\">\\n    <div class=\"col-sm-8 mt-3 mt-md-0\" >\\n        {% include figure.html path=\"https://i.imgur.com/YamQGoe.png\" class=\"img-fluid rounded z-depth-1\" zoomable=false %}\\n    </div>\\n</div>\\n<div class=\"caption\" >\\n    Both billing and \"Pipelines\" dashboard suggest that indeed, this is what I need. Both instances are powered down while not in use.\\n</div>\\n\\nThe question I\\'ll try to answer today is whether or not GPU acceleration is available here. I mean, it should be. Otherwise this \"Vertex AI Pipelines\" would just be functionally identical to Cloud Run. So, wish me luck. \\n\\n\\n## 2022, Jul 26th, 21:10 - TornikeO\\n\\nI\\'m currently working on building a human-shape-from-image paper, called SHAPY from CVPR2022. It\\'s a solid paper, with solid results, however, it is an absolute PITA to install and run. The documentation is lacking. Concrete example of this is the fact that you need to install `libturbojpeg` and there is no mention of this anywhere, except the issues tab (what would it take to update a `README.md`, like, 20 minutes?). Worse still is the fact that the library depends on OpenPose. A library that\\'s extremely hard to use in headless linux environment. \\n\\nSo, lo and behold, I manged to get everything working. How? \\n\\n1. I install shapy repo and follow instructions precisely (if you get \"import attributes\" error, that means you need to look for the `export $PATH` command in the install guide and run that). Another error comes from a missing dependency, libturbojpeg. I did `sudo apt-get install libturbojpeg`, or a variation of the package\\'s name, can\\'t recall.\\n2. Did a sample run on the `regression/demo.py`. To run on new images, we have to change openpose dataset location arg.\\n4. SHAPY depends on JSON pose keypoints. This is generated by openpose. Here\\'s where the pain begins - openpose doesn\\'t provide linux builds. So you have to build one yourself within or outside of docker. \\n5. I managed to run openpose within a docker container, using [this guide](https://janbkk10.medium.com/build-to-openpose-docker-on-ssh-server-5603874834e9). This took over 3 hours. :/ Installation takes good 30 minutes or more on 12 vCPU machine.\\n6. Once done, find openpose docs for CLI usage, you will need to use `--write_json` arg for this. \\n7. Use FFMPEG to extract frames from a video\\n8. Use openpose to create JSONs for frames\\n9. Use SHAPY regressor to predict shape -> ply file\\n10. Use shapy virtual measurements tool to extract measurements from generated shapes. Ply -> Measurements (30x)\\n11. Average measurements out for more accuracy. Average measurements.\\n\\nTODO: Check out polycam. \\n\\n## 2022, Jul 27th, 23:10 - TornikeO\\n\\nHere\\'s a summary of what I learned today. When using \"official\" pytorch docker images, \"sudo apt update\" command will error out \\nwith a something about missing GPG Keys for NVIDIA packages. This is fixed by the following two lines:\\n\\n```Dockerfile\\nRUN rm /etc/apt/sources.list.d/cuda.list\\nRUN rm /etc/apt/sources.list.d/nvidia-ml.list\\n```\\n\\nThe source of this fix is [on github](https://github.com/NVIDIA/nvidia-docker/issues/1632#issuecomment-1112667716).\\n\\nSecond, the Dockerfile build process fails at the when at the `mesh-mesh-interaction` directory, we execute `setup.py`. I tried two separate base images for this, one from pytorch docker and other from nvcr repository, and both fail, saying `No CUDA runtime is found, using CUDA_HOME=’/usr/local/cuda’`. I have yet to find a solution to this problem. Maybe drivers are missing?\\n\\nSeveral things to try out tomorrow:\\n- Omit faulty line and instead manually run `nvidia-smi` within the container. Does this output the expected summary?\\n- Start from Ubuntu base image. Install everything by hand. Commit as a new image. Yeah, this sounds exhausting. \\n\\n[x] Docker image for both OpenPose\\n[ ] Docker image for SHAPY \\n\\n## 2022, Jul 28th, 19:41 - TornikeO\\n\\nToday I had to concentrate on *proving* that SHAPY has accurate measurements. So, I was given a sample video of one of remote colleagues moving around within a video. All I had to do, is extract some frames from the video, run them through first OpenPose and then through SHAPY and then present the results.\\n\\nSo, that\\'s what I did. I was given a 2000px by 1600px video, which, let\\'s be real, is of too high a resolution. I needed an easily automatized way of extracting the frames from the video as well as resizing the resulting frame\\'s *longest* size to the default 600px (that\\'s what the sample SHAPY images already come with). Here\\'s an FFMPEG command that does all that in one go:\\n\\n```bash\\nffmpeg -i inp.avi -filter:v scale=300:-1 -r 1 images/img_%03d.png\\n```\\n\\nIn case you also need to flip the frames around, you can use this\\n\\n```bash\\nffmpeg -i inp.avi -vf transpose=1 -r 5 images/img_%03d.png\\n```\\n\\nYou also can combine all these flags that make sense together. Giving you the ability to easily define preprocessing pipeline.\\n\\nSecond, I forgot that OpenPose in a headless server (docker), requires special args to not fail (the error  logs are not that helpful, although it does mention word \"display\" at some point).\\n\\nFirst, we instantiate the container\\n```bash\\n# Note that in my case the main repo rests in /home/jupyter/shapy2 path\\n# Your\\'s might be different\\n# Also, --gpus all is important!\\n\\ndocker run --gpus all -it --rm \\\\\\n    -v /home/jupyter/shapy2:/workdir \\\\\\n    tornikeo/openpose /bin/bash\\n```\\n\\nEnter the docker env and run the following command (assuming you do have images )\\n\\n```bash\\n# Within tornikeo/openpose, with --gpus all arg\\n# --write_images is optional. I like to keep it to sanity check keypoints\\n# Make sure that that all the limbs are actually there!\\n./build/examples/openpose/openpose.bin \\\\\\n    --display 0 --render_pose 0 \\\\\\n    --image_dir /workdir/samples/custom/images/ \\\\\\n    --write_json /workdir/samples/custom/openpose/ \\\\\\n    --write_images /workdir/samples/custom/openpose/keypoint_sanity_check\\n```\\n\\n\\nThe above command will create json keypoint files. However, it\\'s not ideal. Here\\'s why: Each created file will have an extra \\'_keypoints\\' at the end of the file name. To remove it efficiently, **install the `rename` apt package**. I did it outside the docker env, since that doesn\\'t concern openpose. \\n\\n```bash\\n# OUTSIDE of the docker container, in the keypoints folder.\\n# To rename the results to match starter files\\n# You might need to install this package!\\n# sudo apt install rename\\nrename  \\'s/_keypoints//\\' *\\n```\\n\\nNow we have everything we need for SHAPY to run. We have images and keypoints. Let\\'s run it. I have created a folder `custom` and arranged it to fully mimic the original SHAPY folder structure. That way, you only have to add `custom/` to three places to avoid overwriting SHAPY\\'s included stuff.\\n\\n```bash\\n# You are at SHAPY/\\nexport PYTHONPATH=$PYTHONPATH:$(pwd)/attributes/\\ncd regression\\npython demo.py --save-vis true \\\\\\n    --save-params true \\\\\\n    --save-mesh true \\\\\\n    --split test \\\\\\n    --datasets openpose \\\\\\n    --output-folder ../samples/custom/shapy_fit/ \\\\\\n    --exp-cfg configs/b2a_expose_hrnet_demo.yaml \\\\\\n    --exp-opts output_folder=../data/trained_models/shapy/SHAPY_A \\\\\\n        part_key=pose \\\\\\n        datasets.pose.openpose.data_folder=../samples/custom/ \\\\\\n        datasets.pose.openpose.img_folder=images  \\\\\\n        datasets.pose.openpose.keyp_folder=openpose \\\\\\n        datasets.batch_size=1  \\\\\\n        datasets.pose_shape_ratio=1.0\\n```\\n\\nNow let\\'s do measurements:\\n\\n```bash\\n# You are at SHAPY/\\nexport PYTHONPATH=$PYTHONPATH:$(pwd)/attributes/\\ncd measurements\\npython virtual_measurements.py \\\\\\n    --input-folder ../samples/custom/shapy_fit/ \\\\\\n    --output-folder=../samples/custom/virtual_measurements/\\n```\\nNow, the default image outputs are not going to be helpful. You will need to somehow aggreage the N frames you get (each frame gets it\\'s own shapy estimate).\\n\\n<div class=\"row mt-3\" style=\"justify-content:center;\">\\n    <div class=\"col-sm-8 mt-3 mt-md-0\" >\\n        {% include figure.html path=\"https://i.imgur.com/b9fn695.png\" class=\"img-fluid rounded z-depth-1\" zoomable=false %}\\n    </div>\\n</div>\\n<div class=\"caption\" >\\n    Default image that is generated isn\\'t too helpful, when you have many frames.\\n</div>\\n\\nFor that, we modify the `measurements/virtual_measurements.py` file a little. We import pickle and add the following lines starting at line 83:\\n\\n```python\\n    # print result <-- this is line 83\\n    mmts_str = \\'    Virtual measurements: \\'\\n    mmts_dict = {}\\n    for k, v in measurements.items():\\n        value = v[\\'tensor\\'].item()\\n        unit = \\'kg\\' if k == \\'mass\\' else \\'m\\'\\n        mmts_str += f\\'    {k}: {value:.2f} {unit}\\'\\n        mmts_dict[k] = value\\n        \\n    with open(osp.join(demo_output_folder, \\n                        npz_file.replace(\\'npz\\', \\'pickle\\')), \\'wb\\') as handle:\\n        pickle.dump(mmts_dict, handle)\\n```\\n\\nWe save measurements with `mmts_dict[k] = value` and export it as a pickle. This is done for each frame. So, we need another script, which does the following:\\n\\n```python\\nimport pickle\\nimport os\\nimport glob\\nimport pandas as pd\\nmeas = []\\nfor file in glob.glob(\\'*.pickle\\'):\\n    meas.append(pickle.load(open(file,\\'rb\\')))\\nmeas = pd.DataFrame(meas)\\nprint(meas.describe())\\n```\\n\\nThat\\'s it for today. We have an estimate and we have statistics. \\n\\n## 2022, Jul 29th, 22:05 - TornikeO\\nWe have little in the way of discoveries today. However, I did manage to make an **end-to-end** jupyter notebook that inputs a video and outputs a measurement, based on the $$N$$ evenly-spaced frames from it.\\n\\nSome issues were definitely encountered, though. Let\\'s go over the major ones.\\n\\nFirst off, the design. It is tempting to try to write the most general program for the given task, and just the thought of every possiblity might make you a *very* inefficient programmer. So, instead, what I think about is the functonality that 95% of the usecases are going to be happy with? Bonus points if you try to write the code so that it\\'s easy to extend and modify (depends on as little as possible and is as easy to understand as possible). I went with the good \\'ol main for loop design. After all, we will be given a list of videos, and we gotta measure the whole lot of \\'em. So, the the pseudo-code looks like this:\\n\\n```python\\nfor video in videos:\\n    frames = ffmpeg_extract_frames(video, num_frames)\\n    keypoints = openpose_keypoints(frames)\\n    shapes_3d = shapy_estimate_shape(frames, keypoints)\\n    all_meas = virtual_meas_tool(shapes_3d)\\n    print(\\'Video: \\', video, \\'Final meas: \\', mean(all_meas))\\n```\\n\\nEasy, right? Well, almost. The FFMPEG is easy enough:\\n\\n```bash\\nffmpeg -i {input_video} \\\\\\n    -hide_banner \\\\\\n    -filter:v scale=300:-1 \\\\\\n    -vframes {num_frames} \\\\\\n    {img_dir}/img_%03d.jpg\\n```\\n\\nOpenPose lives inside a docker container, so its a bit verbose:\\n\\n```bash\\ndocker run --gpus all \\\\\\n    -it --rm \\\\\\n    -v {output_dir/\\'images\\'}:/workdir/input \\\\\\n    -v {output_dir/\\'openpose\\'}:/workdir/json \\\\\\n    -v {output_dir/\\'openpose\\'/\\'pose\\'}:/workdir/pose \\\\\\n    tornikeo/openpose:latest \\\\\\n        ./build/examples/openpose/openpose.bin \\\\\\n            --display 0 \\\\\\n            --image_dir /workdir/input \\\\\\n            --write_json /workdir/json/ \\\\\\n            --write_images /workdir/pose\\n```\\n\\nThere is a potential bug source here - sometimes openpose doesn\\'t detect all keypoints, leading to weird final measurements in SHAPY. To prevent that, we\\'ll have to filter the outputs, making sure this doesn\\'t happen.\\n\\nWe also need to replace the the extra `_keypoints` suffix in file names like so:\\n\\n```py\\nfor kpt in (output_dir/\\'openpose\\').glob(\\'*.json\\'):\\n    kpt.rename(str(kpt).replace(\\'_keypoints\\',\\'\\'))\\n```\\n\\nNow comes the hard part. I have to different conda environment in the following cell, since, at the time, I didn\\'t know how to change notebook kernel in jupyterlab without restarting it. Here\\'s the trick to do it:\\n\\n```bash\\n%%bash -s \"$output_dir\" #<-- more about this below!\\neval \"$(command conda \\'shell.bash\\' \\'hook\\' 2> /dev/null)\"\\nexport PYTHONPATH=$PYTHONPATH:$(pwd)/attributes/\\nconda activate shapy2\\ncd regressor\\npython demo.py --save-vis true \\\\\\n    --save-params true \\\\\\n    --save-mesh true \\\\\\n    --split test \\\\\\n    --datasets openpose \\\\\\n    --output-folder $1/shapy_fit \\\\\\n    --exp-cfg configs/b2a_expose_hrnet_demo.yaml \\\\\\n    --exp-opts output_folder=../data/trained_models/shapy/SHAPY_A \\\\\\n        part_key=pose datasets.pose.openpose.data_folder=$1 \\\\\\n        datasets.pose.openpose.img_folder=images  \\\\\\n        datasets.pose.openpose.keyp_folder=openpose \\\\\\n        datasets.batch_size=3  \\\\\\n        datasets.pose_shape_ratio=1.0\\n```\\n\\nI copied this from a StackOverflow answer, saw that it worked (`which python` showed the correct path) and never looked back :smiley:. The rest is just the same as in the base repository. Except for one caveat! You see, I\\'m using a IPython environment. And I need to somehow communicate python variables to `%%bash` cells. Inline bash calls, with `!` operator, is easy, you just wrap your variable in `{curly_brackets}` and it just works. This fails, however, in bash cells. So, you have to use the above approach add `-s \"$var_name\"` after the `%%bash` line. You can then access that variable as a `$1` temporary variable within the cell. Neat.\\n\\nNext comes the measurements tool:\\n\\n```bash\\n%%bash -s \"$output_dir\"\\neval \"$(command conda \\'shell.bash\\' \\'hook\\' 2> /dev/null)\"\\nexport PYTHONPATH=$PYTHONPATH:$(pwd)/attributes/\\nconda activate shapy2\\ncd measurements\\npython virtual_measurements.py \\\\\\n    --input-folder=$1/shapy_fit/ \\\\\\n    --output-folder=$1/virtual_measurements/\\n```\\n\\nFinally, we do the measurements aggregation, like so:\\n\\n```py\\nimport pandas as pd\\nimport pickle\\nmeas = []\\nfor file in (output_dir/\\'virtual_measurements\\').glob(\\'*.pickle\\'):\\n    meas.append(pickle.load(open(file,\\'rb\\')))\\nmeas = pd.DataFrame(meas)\\nmeas.describe()\\n```\\n\\nRemember, I modified the `virtual_measurements.py` to pickle the useful vars and save the separately. So, there you have it, the whole pipeline, end to end!\\n\\nThe next step would be to also dockerize the SHAPY and reduce the complexity and fragility of this pipeline. \\n\\n## 2022, Jul 31th, 22:24 - TornikeO\\nI finally managed to dockerize [SHAPY](https://github.com/muelea/shapy/). Tough task. Here\\'s what I learned.\\n\\nThere are apps that need to access nvidia drivers during the docker build phase. Usually, these apps come with CUDA or C++ source code, and dockerizing them the usual way is not going to work. You will get errors like `No CUDA runtime is found, using CUDA_HOME=\\'/usr/local/cuda\\'`. This error in my case was solved by [this stackoverflow answer](https://stackoverflow.com/a/61737404/14142345). A summary is [provided here as well](https://github.com/NVIDIA/nvidia-docker/wiki/Advanced-topics#default-runtime). In a nutshell, you will need to edit the `/etc/docker/daemon.json` to look like:\\n\\n```json\\n{\\n    \"runtimes\": {\\n        \"nvidia\": {\\n            \"path\": \"nvidia-container-runtime\",\\n            \"runtimeArgs\": []\\n        }\\n    },\\n    \"default-runtime\": \"nvidia\"\\n}\\n```\\n\\nYou have to edit this manually into the `daemon.json` file and then do `sudo systemctl restart docker` and, then just re-run your `docker build` commands. The error will be resolved.\\n\\nSecond, by viewing error logs (you can do that by using `docker build -t myimage . 2>&1 | tee logs.text` to both view progress in-terminal and also output a log file). \\n\\nNext, I needed to use `nvcc` during the build process. I was using the `cuda:11.7.0-base-ubuntu20.04` image for the build. Turns out that this image doesn\\'t have `nvcc` (and is thus only 0.6GB in size). Changed the first line into:\\n\\n```dockerfile\\nfrom nvidia/cuda:11.7.0-devel-ubuntu20.04\\n```\\n\\nAnd the error was resolved.\\n\\nThen, I encountered several more errors for missing libraries. For example:\\n\\nPython Open3D import error:\\n\\n```python\\n>>> import open3d\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\n  File \"/usr/local/lib/python3.8/dist-packages/open3d/__init__.py\", line 56, in <module>\\n    _CDLL(str(next((_Path(__file__).parent / \\'cpu\\').glob(\\'pybind*\\'))))\\n  File \"/usr/lib/python3.8/ctypes/__init__.py\", line 373, in __init__\\n    self._handle = _dlopen(self._name, mode)\\nOSError: libGL.so.1: cannot open shared object file: No such file or directory\\n```\\nCan be solved with \\n\\n```bash\\napt-get install -y libgl1-mesa-glx\\n```\\n\\nEGL library error:\\n\\n```python\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.8/dist-packages/OpenGL/platform/egl.py\", line 67, in EGL\\n    return ctypesloader.loadLibrary(\\n  File \"/usr/local/lib/python3.8/dist-packages/OpenGL/platform/ctypesloader.py\", line 45, in loadLibrary\\n    return dllType( name, mode )\\n  File \"/usr/lib/python3.8/ctypes/__init__.py\", line 373, in __init__\\n    self._handle = _dlopen(self._name, mode)\\nOSError: (\\'EGL: cannot open shared object file: No such file or directory\\', \\'EGL\\', None)\\n```\\n        \\nCan be solved with:\\n\\n```bash\\napt-get install -y libglfw3-dev libgles2-mesa-dev\\n```\\n\\nAnd, finally, `pyglet` error:\\n\\n```python\\n>>> import pyglet\\n>>> \\n>>> lib = pyglet.lib.load_library(\\'GLU\\')\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\n  File \"/usr/local/lib/python3.8/dist-packages/pyglet/lib.py\", line 168, in load_library\\n    raise ImportError(\\'Library \"%s\" not found.\\' % names[0])\\nImportError: Library \"GLU\" not found.\\n```\\n        \\nCan be solved with \\n\\n```bash\\napt install freeglut3-dev\\n```\\n\\nAfterwards:\\n\\n```python\\n>>> import pyglet\\n>>> lib = pyglet.lib.load_library(\\'GLU\\')\\n>>> lib\\n<CDLL \\'libGLU.so.1\\', handle cc2360 at 0x7faa081d3460>\\n>>>\\n```\\nWith these, I assembled a working, ready-to-use docker image at [`tornikeo/shapy`](https://hub.docker.com/repository/docker/tornikeo/shapy). Image is **10GB** (!) in size. So, beware. It also doesn\\'t contain model\\'s weights. Those have to be downloaded from the [website](https://shapy.is.tue.mpg.de/). \\n\\n## 2022, Aug 9th, 12:24 - TornikeO\\n\\nIn case you get the following error while doing a `docker run --gpus`: \\n\\n```bash\\ndocker: Error response from daemon: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as \\'legacy\\'\\nnvidia-container-cli: requirement error: unsatisfied condition: cuda>=11.7, please update your driver to a newer version, or use an earlier cuda container: unknown.\\n```\\n\\nChange the command to run the docker to the following:\\n\\n```bash\\ndocker run --gpus all --env NVIDIA_DISABLE_REQUIRE=1 ...\\n```\\n\\nMake sure that within the container, `nvidia-smi` is still accessible.\\n\\n\\nIf for some reason, a GCloud GPU-enabled VM, where you did run the initial driver installation script, can\\'t access GPU,\\nwhen trying to start `docker run --gpus` instance, you will get an error:\\n\\n```bash\\nnvidia-container-cli: initialization error: nvml error: driver not loaded: unknown.\\n```\\n\\nIf this VM also can\\'t execute `nvidia-smi` on host as well, you will have to do the following ([cocumented on GCLOUD](https://cloud.google.com/compute/docs/gpus/install-drivers-gpu#installation_scripts)):\\n\\n```bash\\ncurl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py\\nsudo python3 install_gpu_driver.py\\nsudo nvidia-smi\\n```\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(all_mkd_files[0],'r').read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
